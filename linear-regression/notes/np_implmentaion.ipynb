{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a18f0f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2169f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(x: np.ndarray, y: np.ndarray, w: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error cost for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Input features\n",
    "        y (np.ndarray): True target values\n",
    "        w (float): Weight parameter\n",
    "        b (float): Bias parameter\n",
    "    \n",
    "    Returns:\n",
    "        float: The computed MSE cost\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    predictions = (w * x) + b\n",
    "    cost = (1/(2*m)) * np.sum((y - predictions) ** 2)\n",
    "    return cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0585b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_function(x: np.ndarray, y: np.ndarray, w: float, b: float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate the gradients of the cost function with respect to w and b.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Input features\n",
    "        y (np.ndarray): True target values\n",
    "        w (float): Current weight parameter\n",
    "        b (float): Current bias parameter\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[float, float]: Gradients (dw, db)\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    predictions = (w * x) + b\n",
    "    \n",
    "    # Calculate gradients\n",
    "    dw = (1/m) * np.sum((predictions - y) * x)\n",
    "    db = (1/m) * np.sum(predictions - y)\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89994b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    x: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    learning_rate: float, \n",
    "    iterations: int, \n",
    "    tolerance: float = 1e-6\n",
    ") -> Tuple[float, float, List[float]]:\n",
    "    \"\"\"\n",
    "    Perform gradient descent optimization for linear regression.\n",
    "    \n",
    "    Args:\n",
    "        x (np.ndarray): Input features\n",
    "        y (np.ndarray): True target values\n",
    "        learning_rate (float): Learning rate (alpha)\n",
    "        iterations (int): Maximum number of iterations\n",
    "        tolerance (float, optional): Convergence tolerance. Defaults to 1e-6\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[float, float, List[float]]: Final weight, final bias, and cost history\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    w = 0.0\n",
    "    b = 0.0\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Calculate gradients\n",
    "        dw, db = gradient_function(x, y, w, b)\n",
    "        \n",
    "        # Update parameters\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Calculate and store cost\n",
    "        cost = cost_function(x, y, w, b)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Print progress\n",
    "        if i % 100 == 0:  # Print every 100 iterations\n",
    "            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
    "        \n",
    "        # Check convergence\n",
    "        if i > 0 and abs(cost_history[i] - cost_history[i-1]) < tolerance:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "    \n",
    "    return w, b, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "90fd7c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost = 17.107600\n",
      "Iteration 100: Cost = 0.017117\n",
      "Iteration 200: Cost = 0.012203\n",
      "Iteration 300: Cost = 0.008700\n",
      "Iteration 400: Cost = 0.006202\n",
      "Iteration 500: Cost = 0.004422\n",
      "Iteration 600: Cost = 0.003152\n",
      "Iteration 700: Cost = 0.002247\n",
      "Iteration 800: Cost = 0.001602\n",
      "Iteration 900: Cost = 0.001142\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "w, b, costs = gradient_descent(x, y, learning_rate=0.01, iterations=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
